# Base config file which stores the default parameters which apply to all model configurations
---

# Mode to use for debugging/development; this only uses a few samples in in the train/val dataset
# to quickly run the code
debug_mode: False

# Base directory for output files; do not change this 
output_path: "output/train"

# Experiment name; this will be used for the output directory; use "development" when developing
exp_name: "development"

# Parameters for the dataset class
dataset:
  # Name of dataset
  name: "cifar10"

  # Original image size before any transforms
  image_size: 32

  # Path to the root of the dataset
  root: "/mnt/c/Users/bsele/Documents/Datasets/cifar10"

  # Number of CPU processes the next sample in the dataset; use 0 to only use the main process
  num_workers: 4

# Model to be trained during diffusion
model_name: unet
  
# Log the train progress every n steps
logging_intervals: 20

train:

  # Configurations for learning such as the optmizier and lr scheduler
  learning_config: "learning_config_1"

  batch_size: 4

  # The step to start on; ddpm doesn't seem to use epochs and uses steps instead;
  start_step: 1
  steps: 700000

  # Number of steps to checkpoint after; use 'null' to turn off checkpointing
  ckpt_steps: 5000

  # Number of steps to sample images after
  eval_intervals: 2000

  # Number of samples to generate during evaluation
  num_samples: 16

  # Number of samples to pass through the model during sampling
  sampling_batch_size: 4

  # Path of weights file (.pt) to resume training; use `null` to train a new model from scratch 
  checkpoint_path: /home/bselee/programming/diffusion/output/train/development/2024_11_11-09_40_21_PM/checkpoints/checkpoint0015000.pt
  

# GPU parameters
cuda:

  # List of GPU devices to use
  gpus: [0]

# Reproducibility information
reproducibility:
  seed: 42


# Params for training objects
learning_config_1:
  optimizer: "adam"

  learning_rate: 0.0002 #0.00008 # Lucidrains uses 8e-5 in the README

  # L2 regulartization penalty to add to the loss function: 
  weight_decay: 0.0